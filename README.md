#### ___Incorporating biologically plausible representational constraints into hierarchical models of vision.___

There is a striking correspondence between goal-driven models of sensory processing and primate sensory cortex. Convolutional neural networks (CNNs) trained to perform a sufficiently challenging computational task (e.g. object recognition on [Imagenet](https://www.researchgate.net/profile/Li_Jia_Li/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database/links/00b495388120dbc339000000/ImageNet-a-Large-Scale-Hierarchical-Image-Database.pdf)), learn internal representations that reflect the hierarchical nature of information processing in the ventral visual system ([Yamins & DiCarlo 2016](https://pdfs.semanticscholar.org/b207/16844ee23fc6f8889c12d08d4442a0de1348.pdf)). Edge-detection-like properties emerge in primary visual cortex, as well as the principle layers of CNNs ([Yamins et al. 2014](https://pdfs.semanticscholar.org/b207/16844ee23fc6f8889c12d08d4442a0de1348.pdf)), for example, and "face detection" emerges in downstream layers of both systems--qualitative observations that also hold for more formal model comparisons. 

Unlike computational models of vision, however, the cortex does not receive raw pixel inputs. In the vertibrate nervous system, visual information is transformed, principally, within the retina; sensory inputs undergo a 10-fold dimensionality reduction, before being passed to the lateral geniculate nuculeus of the thalamus, and then to primary visual cortex. This multi-stage "preprocessing" of visual input is reflected, perhaps, in the empirical observations that primary visual cortex is best fit by early-to-intermediate layers of goal-driven models of vision ([Cadena et al. 2017](https://www.biorxiv.org/content/early/2017/10/11/201764)), suggesting that prior layers in these models might be performing computations related to pre-cortical neural systems.

To incorporate these biologically plausible, representational constraints into a hierarchical model of visual cortex, we replaced early layers of a goal-driven model of vision (VGG16) with a computational model of the retina. This retinal "front-end" was a 3-layer CNN trained to predict the firing rates of Salamander RGC responses to natural images. We fixed the weights of this retinal front-end and fine-tuned the weights of the remaining “cortical” VGG16 on ImageNet. To evaluate our model, we compared the ability of the original VGG16 and the retinal-VGG16 (ret-VGG) to predict neural responses in macaque V4 and IT. 

Given the same classification accuracy on object categorization, we predict that the internal representations ret-VGG learns will better fit neural data than the original VGG16. That is, incorporating biologiclaly plausible constraints on the network's architecture will increase our ability to predict neural data downstream. Initial results indicate that a retinal frond-end does not improve neural fits to IT, but this is hardly a fair comparision, as the model has yet to match the original VGG16 on behavioral performance. Initial learning curves suggest that, even with this frankenstein-like amalgam, ret-VGG is learning to solve the task. We hope to perform a formal model comparision once ret-VGG has been sufficiently trained. 



